{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f622596",
   "metadata": {},
   "source": [
    "# VectorDB\n",
    "\n",
    "Vector databases are optimized for semantic search. They use **ANN (Approximate Nearest Neighbor)** algorithms, which trade a bit of accuracy for significantly faster performance compared to exact **KNN (k-Nearest Neighbors)**. ANN typically operates in *O(N log N)* time, while KNN is *O(N)*.\n",
    "\n",
    "### VectorDB vs RDBMS\n",
    "\n",
    "* **RDBMS** stores structured data (rows and columns) and relies on exact keyword matching.\n",
    "* **VectorDB** stores unstructured data (text, images, audio, video) as vector embeddings and enables similarity-based search.\n",
    "* VectorDBs are faster for semantic search and are critical in GenAI and **RAG (Retrieval-Augmented Generation)** systems.\n",
    "\n",
    "### Indexing Techniques in VectorDB\n",
    "\n",
    "* **Flat**: Brute-force search.\n",
    "* **LSH (Locality-Sensitive Hashing)**: Groups vectors into hash buckets.\n",
    "* **IVF (Inverted File Index)**: Partitions vectors into clusters; **IVFPQ** further compresses each cluster.\n",
    "\n",
    "  * *Note:* Borderline vectors may trigger search in nearby clusters.\n",
    "* **HNSW (Hierarchical Navigable Small World)**: Organizes vectors in a multi-layer graph for fast navigation across similar vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f2b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b31a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader = pdfplumber.open(\"../Data/Uber-2024-Annual-Report.pdf\")\n",
    "len(pdf_reader.pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f7150",
   "metadata": {},
   "source": [
    "#### Chunking Strategies\n",
    "- Fixed Size chunking - Fixed length\n",
    "- Sentence based chunking - \\n\n",
    "- Paragraph based Chunking - \\n\\n\n",
    "- Page based Chunking\n",
    "- Token based chunking - Fixed length of tokens rather than words\n",
    "- Sliding window chunking - Overlaps some content from previous chunk\n",
    "- Hierarhical Chunking - Breaks down documents at multiple levels, such as sections, subsections, and paragraphs\n",
    "- Content-Aware Chunking - Chunking text at paragraph level and tables as seperate entities\n",
    "- Table aware Chunking\n",
    "- Keyword based Chunking - Introduction, Conclusion, Summary these are chunked\n",
    "- Hybrid Chunking - Using different Chunking strategies based on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4653a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content = []\n",
    "document_name = \"\".join(pdf_reader.stream.name.split(\"/\")[-1].split(\".\")[:-1])\n",
    "for i, page in enumerate(pdf_reader.pages):\n",
    "    text_page = page.extract_text()\n",
    "\n",
    "    split_text = text_page.split(\"\\n\")\n",
    "\n",
    "    for text in split_text:\n",
    "        if len(text.split(\" \")) > 10:\n",
    "            text_content.append({\n",
    "                \"type\" : \"text\",\n",
    "                \"document\": document_name,\n",
    "                \"page\": f\"{i+1}\",\n",
    "                \"content\": text\n",
    "            })\n",
    "\n",
    "text_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace2054",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content = []\n",
    "\n",
    "def find_middle_newline(s):\n",
    "    # Step 1: Find all indexes of '\\n'\n",
    "    newline_indices = [i for i, char in enumerate(s) if char == '\\n']\n",
    "    \n",
    "    if not newline_indices:\n",
    "        return None  # No newline found\n",
    "    \n",
    "    # Step 2: Find the middle index\n",
    "    middle_index = len(newline_indices) // 2\n",
    "    \n",
    "    # Step 3: Return the position of the middle '\\n'\n",
    "    return newline_indices[middle_index]\n",
    "\n",
    "document_name = \"\".join(pdf_reader.stream.name.split(\"/\")[-1].split(\".\")[:-1])\n",
    "for i, page in enumerate(pdf_reader.pages):\n",
    "    text_page = page.extract_text()\n",
    "\n",
    "    if len(text_page.split(\" \")) < 10:\n",
    "        print(f\"Page number: {i+1}, count: {len(text_page.split(\" \"))}\")\n",
    "        continue\n",
    "\n",
    "    if len(text_page) > 5000:\n",
    "        mid_index = find_middle_newline(text_page)\n",
    "        text_content.append({\n",
    "            \"type\" : \"text\",\n",
    "            \"document\": document_name,\n",
    "            \"page\": f\"{i+1}\",\n",
    "            \"split\":f\"0\",\n",
    "            \"content\": text_page[:mid_index]\n",
    "        })\n",
    "\n",
    "        text_content.append({\n",
    "            \"type\" : \"text\",\n",
    "            \"document\": document_name,\n",
    "            \"page\": f\"{i+1}\",\n",
    "            \"split\":f\"1\",\n",
    "            \"content\": text_page[mid_index+1:]\n",
    "        })\n",
    "    else:\n",
    "        text_content.append({\n",
    "                    \"type\" : \"text\",\n",
    "                    \"document\": document_name,\n",
    "                    \"page\": f\"{i+1}\",\n",
    "                    \"split\":f\"0\",\n",
    "                    \"content\": text_page\n",
    "                })\n",
    "\n",
    "text_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d478b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_doc = pd.DataFrame(text_content)\n",
    "text_doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_doc[\"MetaData\"] = text_doc.apply(lambda x: {\"Document\": x[\"document\"], \"Page\": x[\"page\"], \"Split\": x[\"split\"], \"Type\": x[\"type\"]}, axis=1)\n",
    "text_doc = text_doc.drop([\"type\", \"document\", \"page\"], axis=1)\n",
    "text_doc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ecb876",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "only_text = text_doc[\"content\"].tolist()\n",
    "embeddings = embedding_model.encode(only_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755192e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Chroma_DB_Path = \"../Store/2_VectorDB\"\n",
    "COLLECTION_NAME = \"uber_revenue\"\n",
    "\n",
    "# chroma_client = chromadb.Client(Settings(\n",
    "#     persist_directory=Chroma_DB_Path,\n",
    "#     anonymized_telemetry=False\n",
    "# ))\n",
    "\n",
    "chroma_client = PersistentClient(path=Chroma_DB_Path)\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823a25a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = text_doc[\"MetaData\"].apply(lambda x: f\"{x['Document']}_p{x['Page']}_s{x['Split']}\")\n",
    "ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bc9340",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=text_doc['content'].tolist(),\n",
    "    metadatas=text_doc['MetaData'].tolist(),\n",
    "    ids=ids.tolist()\n",
    ")\n",
    "print(\"Successfully stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b47bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "caching = []\n",
    "cache_emd = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68981690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chroma_results(query):\n",
    "    query_emd = embedding_model.encode([query])\n",
    "    \n",
    "    if len(cache_emd) > 0:\n",
    "        cache_emd_array = np.vstack(cache_emd) \n",
    "        similarities = cosine_similarity(query_emd, cache_emd_array)\n",
    "        best_match_indexes = [np.argmax(item) for item in similarities]\n",
    "\n",
    "        if len(best_match_indexes) > 0 and similarities[0][best_match_indexes[0]] > 0.8:\n",
    "            print(f\"Returning from query: {caching[best_match_indexes[0]][\"query\"]} cache with score: {similarities[0][best_match_indexes[0]]:.4f}\")\n",
    "            return caching[best_match_indexes[0]][\"results\"]\n",
    "    \n",
    "\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=3\n",
    "    )\n",
    "\n",
    "    caching.append({\"query\": query, \"results\": results}) \n",
    "    cache_emd.append(query_emd)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4681dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the revenue of Uber?\"\n",
    "results = get_chroma_results(query=query)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e90c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the profit of Uber?\"\n",
    "results = get_chroma_results(query=query)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bbb07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the loss of Uber?\"\n",
    "results = get_chroma_results(query=query)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d8f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much degrade for Uber?\"\n",
    "results = get_chroma_results(query=query)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much negative margin for Uber?\"\n",
    "results = get_chroma_results(query=query)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c675cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the margin for Uber?\"\n",
    "results = get_chroma_results(query=query)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f13f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What much money Uber made?\"\n",
    "results = get_chroma_results(query=query)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b124802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniforge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
