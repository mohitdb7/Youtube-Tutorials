{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e91b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import cohere\n",
    "from groq import Groq\n",
    "\n",
    "import tqdm\n",
    "import time\n",
    "import requests\n",
    "import PIL\n",
    "import io\n",
    "\n",
    "import base64\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89cfcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5887f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5cd33ba-ae1a-42a8-a254-d85e690d9870_2741x1541.png\"]\n",
    "image_folder_path = os.path.join(os.getcwd(), \"img\")\n",
    "os.makedirs(image_folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8453704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_image_paths = []\n",
    "\n",
    "for url in tqdm.tqdm(image_paths):\n",
    "    img_path = os.path.join(image_folder_path, f\"1.{url.split(\".\")[-1]}\")\n",
    "    if not os.path.exists(img_path):\n",
    "        response = requests.get(url, timeout=10)  # timeout is a good practice\n",
    "        response.raise_for_status()\n",
    "        with open(img_path, \"wb\") as fOut:\n",
    "            fOut.write(response.content)\n",
    "        print(f\"Downloaded: {img_path}\")\n",
    "    else:\n",
    "        print(f\"Already exists: {img_path}\")\n",
    "    \n",
    "    local_image_paths.append(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeac8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "cohere_key = os.getenv(\"COHERE_API_KEY\")\n",
    "print(f\"{cohere_key[:5]}...{cohere_key[-5:]}\")\n",
    "co_client = cohere.ClientV2(api_key=cohere_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize too large images\n",
    "def resize_image(pil_image) -> Image.Image:\n",
    "    max_pixels = 1568*1568  #Max resolution for images\n",
    "    org_width, org_height = pil_image.size\n",
    "\n",
    "    # Resize image if too large\n",
    "    if org_width * org_height > max_pixels:\n",
    "        scale_factor = (max_pixels / (org_width * org_height)) ** 0.5\n",
    "        new_width = int(org_width * scale_factor)\n",
    "        new_height = int(org_height * scale_factor)\n",
    "        # resize is safer than thumbnail. Thumbnail updates the image inplace\n",
    "        pil_image = pil_image.resize((new_width, new_height)) # pil_image.thumbnail((new_width, new_height))\n",
    "        \n",
    "\n",
    "    return pil_image\n",
    "\n",
    "# Convert images to a base64 string before sending it to the API\n",
    "def base64_from_image(img_path):\n",
    "    \"\"\"Convert the image at path to Base64 Image\"\"\"\n",
    "\n",
    "    # Check if the image is a path or PIL image\n",
    "    if isinstance(img_path, str): # If the image is path\n",
    "        pil_image = PIL.Image.open(img_path)\n",
    "        img_format = pil_image.format if pil_image.format else \"PNG\"\n",
    "    else:\n",
    "         pil_image = img_path\n",
    "\n",
    "\n",
    "    # Resize the image to max_pixel\n",
    "    pil_image = resize_image(pil_image)\n",
    "\n",
    "    # Read the image as buffer and convert to base64\n",
    "    with io.BytesIO() as img_buffer:\n",
    "        pil_image.save(img_buffer, format=img_format)\n",
    "        img_buffer.seek(0)\n",
    "        img_data = f\"data:image/{img_format.lower()};base64,\"+base64.b64encode(img_buffer.read()).decode(\"utf-8\")\n",
    "\n",
    "    return img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb25dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_image_with_cohere(co, img_path):\n",
    "    \"\"\"Embed Image using the cohere vision model\"\"\"\n",
    "    # Get the base64 representation of the image\n",
    "    print(img_path)\n",
    "    base64_uri = base64_from_image(img_path)\n",
    "    api_input_document = {\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": base64_uri },\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    resp = co.embed(\n",
    "        model=\"embed-v4.0\",\n",
    "        input_type=\"search_document\",\n",
    "        embedding_types=[\"float\"],\n",
    "        inputs=[api_input_document],\n",
    "    )\n",
    "\n",
    "    return {\"embedding\":resp, \"base64\":base64_uri}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ebc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = [embed_image_with_cohere(co=co_client, img_path=img_path) for img_path in local_image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d0a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_embeddings[0][\"embedding\"].embeddings.float[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44518642",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4000302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the multimodal content for the API call\n",
    "content = [\n",
    "    {\n",
    "        \"type\": \"text\", \n",
    "        \"text\": \"Describe this image in detail.\" # Your text prompt\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            # The data URI contains the image data and its type\n",
    "            \"url\": image_embeddings[0][\"base64\"] \n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "chat_completion = groq_client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": content\n",
    "                }\n",
    "            ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_llm_response(response: str):\n",
    "    from rich.console import Console\n",
    "    from rich.markdown import Markdown\n",
    "    from rich.json import JSON\n",
    "\n",
    "    console = Console()\n",
    "    try:\n",
    "        console.print(JSON(response))\n",
    "    except Exception:\n",
    "        console.print(Markdown(response))\n",
    "\n",
    "# --- Step 8: Display the response ---\n",
    "response_text = chat_completion.choices[0].message.content\n",
    "print_llm_response(f\"\\nResponse:\\n{response_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55a04fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19348742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfd9042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11b12fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0077220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several images from https://www.appeconomyinsights.com/\n",
    "images = {\n",
    "    \"tesla.png\": \"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbef936e6-3efa-43b3-88d7-7ec620cdb33b_2744x1539.png\",\n",
    "    \"netflix.png\": \"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23bd84c9-5b62-4526-b467-3088e27e4193_2744x1539.png\",\n",
    "    \"nike.png\": \"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5cd33ba-ae1a-42a8-a254-d85e690d9870_2741x1541.png\",\n",
    "    \"google.png\": \"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F395dd3b9-b38e-4d1f-91bc-d37b642ee920_2741x1541.png\",\n",
    "    \"accenture.png\": \"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08b2227c-7dc8-49f7-b3c5-13cab5443ba6_2741x1541.png\",\n",
    "    \"tecent.png\": \"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ec8448c-c4d1-4aab-a8e9-2ddebe0c95fd_2741x1541.png\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb39e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from chromadb import PersistentClient\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e9353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(image_paths):\n",
    "    local_image_paths = []\n",
    "    for name, url in tqdm.tqdm(image_paths.items()):\n",
    "        img_path = os.path.join(image_folder_path, f\"{name}\")\n",
    "        if not os.path.exists(img_path):\n",
    "            response = requests.get(url, timeout=10)  # timeout is a good practice\n",
    "            response.raise_for_status()\n",
    "            with open(img_path, \"wb\") as fOut:\n",
    "                fOut.write(response.content)\n",
    "            print(f\"Downloaded: {img_path}\")\n",
    "        else:\n",
    "            print(f\"Already exists: {img_path}\")\n",
    "        \n",
    "        local_image_paths.append({\"name\" : name, \"path\" : img_path})\n",
    "    \n",
    "    return local_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0194e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_image = []\n",
    "documents = []\n",
    "ids = []\n",
    "metadata = []\n",
    "\n",
    "local_image_paths = download_images(image_paths=images)\n",
    "\n",
    "for local_image in local_image_paths:\n",
    "    embedding_with_base64 = embed_image_with_cohere(co=co_client, img_path=local_image[\"path\"])\n",
    "\n",
    "    embeddings_image.append(embedding_with_base64[\"embedding\"].embeddings.float[0])\n",
    "    documents.append(embedding_with_base64[\"base64\"])\n",
    "    ids.append(local_image[\"name\"])\n",
    "    metadata.append({\"type\" : \"image\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c35c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ids)\n",
    "embeddings_image = np.array(embeddings_image, dtype=\"float32\")\n",
    "\n",
    "print(\"💾 Building FAISS HNSW index...\")\n",
    "dim = embeddings_image.shape[1]\n",
    "index = faiss.IndexHNSWFlat(dim, 32)\n",
    "index.hnsw.efConstruction = 40\n",
    "index.add(embeddings_image)\n",
    "faiss.write_index(index, \"../Store/image_db/images_faiss_hnsw.index\")\n",
    "\n",
    "print(\"💾 Storing embeddings in ChromaDB...\")\n",
    "chroma_client = PersistentClient(path=\"../Store/image_db/image_chroma_store\")\n",
    "collection = chroma_client.get_or_create_collection(\"image_store\")\n",
    "collection.add(\n",
    "embeddings=embeddings_image.tolist(),\n",
    "metadatas=metadata,\n",
    "documents=documents,\n",
    "ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a05f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_stores(faiss_path=\"../Store/image_db/images_faiss_hnsw.index\", chroma_path=\"../Store/image_db/image_chroma_store\"):\n",
    "    \"\"\"\n",
    "    Load existing FAISS and Chroma stores if available, otherwise return None placeholders.\n",
    "    \"\"\"\n",
    "    faiss_index = None\n",
    "    chroma_client = None\n",
    "    collection = None\n",
    "\n",
    "    if os.path.exists(faiss_path):\n",
    "        print(f\"📂 Loading existing FAISS index from {faiss_path}\")\n",
    "        faiss_index = faiss.read_index(faiss_path)\n",
    "    else:\n",
    "        print(\"⚠️ No FAISS index found, will build a new one.\")\n",
    "\n",
    "    if os.path.exists(chroma_path) and any(os.scandir(chroma_path)):\n",
    "        print(f\"📂 Loading existing ChromaDB store from {chroma_path}\")\n",
    "        chroma_client = PersistentClient(path=chroma_path)\n",
    "        collection = chroma_client.get_or_create_collection(\"image_store\")\n",
    "    else:\n",
    "        print(\"⚠️ No Chroma store found, will build a new one.\")\n",
    "        chroma_client = PersistentClient(path=chroma_path)\n",
    "        collection = chroma_client.get_or_create_collection(\"image_store\")\n",
    "\n",
    "    return faiss_index, chroma_client, collection\n",
    "\n",
    "faiss_index, chroma_client, collection =  load_or_create_stores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_with_cohere(co, text):\n",
    "    \"\"\"Embed Text using the cohere vision model\"\"\"\n",
    "    resp = co.embed(\n",
    "        model=\"embed-v4.0\",\n",
    "        input_type=\"search_document\",\n",
    "        texts=[text]\n",
    "    )\n",
    "    return resp.embeddings.float[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3b10b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 1\n",
    "def search_in_db(query_emb):\n",
    "    distances, indices = faiss_index.search(query_emb, top_k)\n",
    "    FAISS_ids = [str(i) for i in indices[0] if i > -1]\n",
    "    chromdb_ids = [ids[int(id)] for id in FAISS_ids]    \n",
    "    results = collection.get(ids=chromdb_ids)\n",
    "\n",
    "    for i, meta in enumerate(results[\"metadatas\"]):\n",
    "        item_type = meta[\"type\"]\n",
    "        print(f\"{i+1}. {item_type} (Distance: {distances[0][i]:.4f})\")\n",
    "\n",
    "        if item_type == \"text\":\n",
    "            print(results[\"documents\"][i][:300], \"...\")\n",
    "            # pass\n",
    "        elif item_type == \"image\":\n",
    "            # Show the stored image\n",
    "            img_bytes = base64.b64decode(results[\"documents\"][i].split(\",\")[1])\n",
    "            img = Image.open(io.BytesIO(img_bytes))\n",
    "            plt.imshow(img)\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abec128",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the revenue of tesla?\"\n",
    "query_emb = np.array([embed_text_with_cohere(co_client, query)], dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e4510",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_in_db(query_emb=query_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009c2d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the multimodal content for the API call\n",
    "content = [\n",
    "    {\n",
    "        \"type\": \"text\", \n",
    "        \"text\": query # Your text prompt\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            # The data URI contains the image data and its type\n",
    "            \"url\": image_embeddings[0][\"base64\"] \n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "chat_completion = groq_client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": content\n",
    "                }\n",
    "            ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba18bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = chat_completion.choices[0].message.content\n",
    "print_llm_response(f\"\\nResponse:\\n{response_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f2657e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniforge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
